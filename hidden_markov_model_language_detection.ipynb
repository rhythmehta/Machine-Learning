{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob #importing library to load files\n",
    "\n",
    "#reading texts from files and storing as lists\n",
    "languages = ['A', 'B', 'C'] #languages\n",
    "A = [open(f).read() for f in glob('symbol/language-training-langA*')] #train set\n",
    "B = [open(f).read() for f in glob('symbol/language-training-langB*')] #train set\n",
    "C = [open(f).read() for f in glob('symbol/language-training-langC*')] #train set\n",
    "test = [open(f).read() for f in glob('symbol/language-test-*')] #test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e', 'g', 'k', 'o', 't', 'p', 'A']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting all the unique letters in training languages\n",
    "letters = list({l for word in A+B+C for l in word})\n",
    "letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down Bayes theorem components for our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(lang, txt): #posterior, P(LANGUAGE|TEXT)\n",
    "    return likelihood(txt, lang) * prior(lang) / marginalization(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(txt, lang): #likelihood, P(TEXT|LANGUAGE)\n",
    "    likelihood = prob(txt[0], lang) #prior for the first letter\n",
    "    for i in range(len(txt)-1):\n",
    "        cur, nxt = txt[i], txt[i+1] #combo of two continuos letters at a time\n",
    "        likelihood *= prob(cur + nxt, lang) #P(COMBO|LANGUAGE)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(language): #prior, P(LANGUAGE)\n",
    "    return 1/len(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginalization(txt): #marginalization, P(TEXT), denominator\n",
    "    m = 0\n",
    "    for lang in languages: m += likelihood(txt, lang)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(txt, lang): #P(LETTER|LANGUAGE) or P(2 LETTERS|LANGUAGE) [markov]\n",
    "    if len(txt) == 1: #letter occurence/occurence of all letters\n",
    "        if lang == 'A': return tmA[txt].sum()/tmA.values.sum()\n",
    "        elif lang == 'B': return tmB[txt].sum()/tmB.values.sum()\n",
    "        elif lang == 'C': return tmC[txt].sum()/tmC.values.sum()\n",
    "        else: print(\"Language not found!\")\n",
    "    else: #normalized value from transition matrices\n",
    "        if lang == 'A': return tmA[txt[1]][txt[0]]\n",
    "        elif lang == 'B': return tmB[txt[1]][txt[0]]\n",
    "        elif lang == 'C': return tmC[txt[1]][txt[0]]\n",
    "        else: print(\"Language not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #loading library\n",
    "#markov model using current state to next state probabilities\n",
    "def transitionMatrix(txts): #transition matrix generator for a list of texts\n",
    "    tm = pd.DataFrame(index=letters,columns=letters)\n",
    "    tm = tm.fillna(0) #empty matrix initialization\n",
    "    for txt in txts:\n",
    "        for i in range(len(txt)-1):\n",
    "            cur, nxt = txt[i], txt[i+1]\n",
    "            tm[nxt][cur] += 1\n",
    "    return tm/tm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(txt): #posterior distribution and classification\n",
    "    posts=[]\n",
    "    print(\"Text: \", txt)\n",
    "    for lang in languages:\n",
    "        post = (posterior(lang, txt))\n",
    "        posts.append(post)\n",
    "        print(\"Posterior for language\", lang, \": \" , post)\n",
    "    print(\"Language Class:\", languages[posts.index(max(posts))], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training, transition matrices for all languages\n",
    "tmA = transitionMatrix(A)\n",
    "tmB = transitionMatrix(B)\n",
    "tmC = transitionMatrix(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  pppooootgAookggggtttopAtttkkkeeggeeeeAAAgtkoAkkkkkooppppttppppppgppteoooooottkttttkkkktAAApgookkkkkp\n",
      "Posterior for language A :  2.1395507310550748e-61\n",
      "Posterior for language B :  0.0\n",
      "Posterior for language C :  0.3333333333333333\n",
      "Language Class: C \n",
      "\n",
      "Text:  gooooAAAAAAAAAkkkkkkooooAAAeppppppgeeeeepAAppeektetttgggogptttttttkppAAAApetAeegggtttteetttttppAAAAA\n",
      "Posterior for language A :  1.161719418110752e-68\n",
      "Posterior for language B :  0.0\n",
      "Posterior for language C :  0.3333333333333333\n",
      "Language Class: C \n",
      "\n",
      "Text:  ekogoAgkepokogoppAttpAttgeekApegepApotpAAtpetgAtpopAttpAppAtkokettkgAttggokoogApppepogeApopogetpokog\n",
      "Posterior for language A :  0.3333333333333333\n",
      "Posterior for language B :  0.0\n",
      "Posterior for language C :  3.7046111214919513e-35\n",
      "Language Class: A \n",
      "\n",
      "Text:  AtAgegegegAgegetoAtetAogAooAoeAtegAgeotAoAgoetAteAteoegoeogetekoAoegAoegAtegAgeotAtAoAtetAtgeggAtAto\n",
      "Posterior for language A :  7.248250988955608e-47\n",
      "Posterior for language B :  0.33333333333333326\n",
      "Posterior for language C :  1.994270425693113e-59\n",
      "Language Class: B \n",
      "\n",
      "Text:  popoktgopogettpegAtkegotkogettpApepApegtgApeogokokApetkAgegokpepketpotttpAtkopepototpApettgotokotgok\n",
      "Posterior for language A :  0.3333333333333333\n",
      "Posterior for language B :  0.0\n",
      "Posterior for language C :  4.532606273095044e-47\n",
      "Language Class: A \n",
      "\n",
      "Text:  okopottogttpetgopopotkepeppAgAgetpotpAtgookettgoketkokAgAttpgopAgtkAggAgekottgokAkopApAkopAtgtttpokp\n",
      "Posterior for language A :  0.3333333333333333\n",
      "Posterior for language B :  0.0\n",
      "Posterior for language C :  3.6477206600557125e-40\n",
      "Language Class: A \n",
      "\n",
      "Text:  kteoeoeoAoteoAgeoetoegAgeoekgeteoegetetAgeAoAttAtAteoAtetotAoAoeooegegAoetgeoAtegtAteteoAtAgegAgeAoA\n",
      "Posterior for language A :  1.2208649496036243e-62\n",
      "Posterior for language B :  0.3333333333333333\n",
      "Posterior for language C :  2.3525775161857537e-61\n",
      "Language Class: B \n",
      "\n",
      "Text:  tgAkoggAggAtpetkpAgegAgkepepookepepekogokogetkekekekegegotgekAAkoAtpottktkApAgtpegAkAgAkpopettgApogA\n",
      "Posterior for language A :  0.3333333333333333\n",
      "Posterior for language B :  0.0\n",
      "Posterior for language C :  2.212191396325241e-42\n",
      "Language Class: A \n",
      "\n",
      "Text:  oeeoppppppoooAgggggoAAoookkppeoAApeoAkAAAAAAApkpppgpppkkkkAAAAkkkkeeepeeeeeekktppppkeegggoooooooooAA\n",
      "Posterior for language A :  6.640303044460478e-82\n",
      "Posterior for language B :  0.0\n",
      "Posterior for language C :  0.3333333333333333\n",
      "Language Class: C \n",
      "\n",
      "Text:  ApgotgAApogotgopAgegAtkAgketgAtkpAgottpopopAgpekepokpAokettttpogotpegApAgopetgAgeketkotpokokApopegAt\n",
      "Posterior for language A :  0.3333333333333333\n",
      "Posterior for language B :  0.0\n",
      "Posterior for language C :  3.76865174235176e-47\n",
      "Language Class: A \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for txt in test: #classifying all the test cases\n",
    "    classify(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a weird pattern of obtaining 0.333... posterior for classified class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'g', 'k', 'o', 't', 'p', 'A']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "file = open('speaker.txt').read() #loading file\n",
    "phonemes = list({p for word in file for p in word}) #extracting unique\n",
    "n_phonemes = len(phonemes)\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Probabilties:  [0.3, 0.3, 0.4]\n"
     ]
    }
   ],
   "source": [
    "speakers = ['A', 'B', 'C']\n",
    "n_speakers = len(speakers)\n",
    "#random intial probablities distributed for speakers\n",
    "initialProb = [0.3, 0.3, 0.4] \n",
    "print(\"Initial Probabilties: \", initialProb) #starting probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix: \n",
      " [[0.8 0.1 0.1]\n",
      " [0.1 0.8 0.1]\n",
      " [0.1 0.1 0.8]]\n"
     ]
    }
   ],
   "source": [
    "#initializing transition matrix\n",
    "#assuming uniform priors for interruption\n",
    "transitionMatrix = np.full((n_speakers, n_speakers), 0.1)\n",
    "for i in range(n_speakers):\n",
    "    transitionMatrix[i][i] = 0.8\n",
    "print(\"Transition Matrix: \\n\", transitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission Matrix: \n",
      " [[0.21835967 0.21835967 0.21835967 0.21835967 0.21835967 0.21835967\n",
      "  0.21835967]\n",
      " [0.24487286 0.24487286 0.24487286 0.24487286 0.24487286 0.24487286\n",
      "  0.24487286]\n",
      " [0.02955606 0.02955606 0.02955606 0.02955606 0.02955606 0.02955606\n",
      "  0.02955606]]\n"
     ]
    }
   ],
   "source": [
    "emissionMatrix = np.zeros((n_speakers, n_phonemes)) #initializing emission matrix\n",
    "for i in range(n_speakers):\n",
    "    p = np.random.dirichlet(np.ones(n_phonemes)) #random & normalized\n",
    "    emissionMatrix[i,:]= p[0]\n",
    "print(\"Emission Matrix: \\n\", emissionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "#extracting characters and storing their ASCII value, data prep for HMM model\n",
    "for i in range(len(file)):\n",
    "    data.append(ord(file[i]))\n",
    "#I got an error and jupyter suggested me to reshape data and it worked\n",
    "data = np.asarray(data).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 0 0 1 1 1 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 0 1 1 1 1 1 0 0 0 2 2 2 2 2 2 2 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1\n",
      " 0 2 2 2 2 2 2 2 0 0 1 1 1 0 2 2 2 2 2 2 2 0 1 1 1 1 1 0 0 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 1 1 1 1 1 1 1 0 0 2 2 2 2 2 2 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 2 2 2 2 0 0 1 1 0 1 1 1\n",
      " 1 1 0 2 2 2 2 2 2 2 2 2 0 0 0 1 1 0 2 2 2 2 2 2 2 2 2 2 2 0 1 1 0 1 1 0 0\n",
      " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 1 0 0 2 2 2\n",
      " 2 2 2 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 2 2 2 0 0 2 2 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0\n",
      " 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 0\n",
      " 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 0 1 1 1 1 1 1 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 2 2 2\n",
      " 2 2 2 2 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 2 2 0 1 1 1 0 0 0 1 1 0 2 2 0 1 1\n",
      " 0 2 2 2 2 2 0 0 1 1 0 1 1 0 0 0 0 0 2 2 2 2 2 2 2 0 1 1 0 0 2 2 2 2 2 0 1\n",
      " 0 2 2 0 1 1 1 1 1 0 1 1 1 1 1 0 2 2 2 2 2 2 0 0 0 2 2 2 0 0 1 1 0 0 0 0 0\n",
      " 0 2 2 2 2 2 2 2 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 2 2 2 0 1 1\n",
      " 1 1 1 1 1 0 2 2 2 2 0 1 1 1 1 1 1 1 1 1 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 1 1 0 1 1 1 1 0 0 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 2 2 2 2 2 2 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 2 2 0 0 0 2 2 2 2 0 0 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 0 0 0 0 0 1 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "from hmmlearn import hmm #loading library\n",
    "model = hmm.MultinomialHMM(n_components=n_speakers)\n",
    "model.startprob = initialProb #inital probability\n",
    "model.transmat = transitionMatrix #transition matrix\n",
    "model.emissionprob = emissionMatrix #emission matrix\n",
    "model.fit(data) #data\n",
    "print(model.predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/madsbk/hidden-markov-model-example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
