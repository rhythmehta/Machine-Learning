{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "#loading files\n",
    "jersey = glob.glob(\"C:/Users/rhyth/Documents/CS156/Jersey - n03595614/*\")\n",
    "shirt = glob.glob(\"C:/Users/rhyth/Documents/CS156/Shirt - n04197391/*\")\n",
    "\n",
    "#to store jersey and shirt data\n",
    "jersey_data, shirt_data = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Here I load two folders, which contains all the images, categorized by shirts and jerseys.\n",
    "Then, I open each image in the folder one-by-one and resize them to 20 x 20 and then obtain the flattened array form of the image and then store the respective dataset (jersey or shirt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "for image in jersey: #for jerseys\n",
    "    img = Image.open(image) #open image\n",
    "    img = img.resize((20, 20), resample=0) #resize\n",
    "    img = np.array(img).flatten() #transform to array & flatten\n",
    "    jersey_data.append(img) #store\n",
    "\n",
    "for image in shirt: #for shirts\n",
    "    img = Image.open(image) #open image\n",
    "    img = img.resize((20, 20), resample=0) #resize\n",
    "    img = np.array(img).flatten() #transform to array & flatten\n",
    "    shirt_data.append(img) #store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having the data ready in two different dataset, I classified the elements in dataset to 0: for shirt and 1: for jersey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhyth\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "#labelling the data\n",
    "labelled_jersey = np.asarray([(pic, 1) for pic in jersey_data]) #class 1 for jersey\n",
    "labelled_shirt = np.asarray([(pic, 0) for pic in shirt_data]) #class 0 for shirt\n",
    "#deleting an element which had a problematic shape, manually found it\n",
    "#using a loop over the dataset and array size, required size 1200, rest are invalid\n",
    "#only this one had array length of 400, while others were 1200 (20*20*3, 3 for the RGB) \n",
    "labelled_jersey = np.delete(labelled_jersey, 987, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now preparing (for ML) a merged list of features from jersey and shirt, into X variable and labels for y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhyth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2901: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "#separating X & y and stacking them\n",
    "X = np.append(labelled_jersey[:,0], labelled_shirt[:,0])\n",
    "y = np.append(labelled_jersey[:,1], labelled_shirt[:,1])\n",
    "X = np.stack(i for i in X)\n",
    "y = np.stack(i for i in y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is ready, we will split it into training set and testing set into 80%-20% ratio, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into training & testing with 80:20 ratio respectively\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier\n",
    "Here we use a logistic regression model to train and then test it's accuracy on train & test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#logistic regression model\n",
    "lr = LogisticRegression()\n",
    "#fitting model to train data\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data .score():  0.9776785714285714\n",
      "Test data .score():  0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "#train data accuracy\n",
    "print(\"Train data .score(): \", lr.score(X_train,y_train))\n",
    "#test data accuracy\n",
    "print(\"Test data .score(): \", lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained, 97.8% accuracy on training set which signals overfitting although 54.9% accuracy on test data, which is still okay enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier (SVC)\n",
    "Here I repeat linear classification but with support vectors using RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Classifier Model\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf', gamma='auto') #RBF Kernel\n",
    "\n",
    "#process: model fitting and computing accuracy scores\n",
    "#input: training and testing data with features + labels, svc model\n",
    "#output: accuracy score on training and testing data\n",
    "def fit_metrics(X_train, y_train, X_test, y_test, clf):\n",
    "    clf.fit(X_train,y_train) #model fitting\n",
    "    y_train_pred = clf.predict(X_train) #predicting on train data\n",
    "    y_test_pred = clf.predict(X_test) #predicting on test data\n",
    "    print(\"Train data accuracy_score(): \", accuracy_score(y_train_pred,y_train))\n",
    "    print(\"Test data accuracy_score(): \", accuracy_score(y_test_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data accuracy_score():  0.9776785714285714\n",
      "Test data accuracy_score():  0.48484848484848486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#metrics of SVC model\n",
    "fit_metrics(X_train, y_train, X_test, y_test, clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Representation: Principal Component Analysis (PCA)\n",
    "Now, we reduce the data into 2 principal components and then fit our SVC model and measure accuracy. I chose 2 after trying with different n_components value, because only the first two had major explained variance ratio. Example, with 10 components I got the explained variance ratio as \n",
    "array([0.30917742, 0.15128824, 0.04738025, 0.03173914, 0.02739319, 0.02393147, 0.02155885, 0.01492888, 0.01339326, 0.01066094])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analysis decomposition of data\n",
    "#train and test data PCA transformation\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_train) #PCA fitting\n",
    "X_train_PCA = pca.transform(X_train) #PCA transformation\n",
    "X_test_PCA = pca.transform(X_test) #PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data accuracy_score():  0.9776785714285714\n",
      "Test data accuracy_score():  0.48663101604278075\n"
     ]
    }
   ],
   "source": [
    "#Metrics of SVC trained model with PCA transformed data\n",
    "fit_metrics(X_train_PCA, y_train, X_test_PCA, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30917742, 0.15128824])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA Explained Variance Ratio, only the first 2 had major ratio\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get again ~ 97% accuracy on training but decreased test accuracy of 48.7%. PCA did not improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Representation: Linear Discriminant Analysis (LDA)\n",
    "Similarly, we attempt LDA to reduce our data and then fit SVC model to obtain accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhyth\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "#Linear Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_train_LDA = lda.fit_transform(X_train, y_train)\n",
    "X_test_LDA = lda.fit_transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data accuracy_score():  0.8705357142857143\n",
      "Test data accuracy_score():  0.5098039215686274\n"
     ]
    }
   ],
   "source": [
    "#F#Metrics of SVC trained model with LDA transformed data\n",
    "fit_metrics(X_train_LDA, y_train, X_test_LDA, y_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 87% accuracy on train data, and our test score accuracy is 50% which signals overfitting and also is less than 54% (the highest score) we got for linear classifier without any data reduction. Anyway, this model is no better than taking a guess."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
